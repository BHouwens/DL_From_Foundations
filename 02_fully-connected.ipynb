{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "02_fully-connected.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BHouwens/DL_From_Foundations/blob/main/02_fully-connected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCtWCP44mKgb"
      },
      "source": [
        "# **Forward and Backward Passes**\n",
        "\n",
        "In this notebook we'll be looking at setting up the forward and backward passes for the fully connected model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPFZCnLOmKgc"
      },
      "source": [
        "## **Colab Setup**\n",
        "The setup structure for this will depend on the environment. I'm assuming a Google Colab environment in this case, which will require the following setup from the Github repo, and assumes that the repo has already been cloned into Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDmZQwIumKgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b3ecbd-3527-44b6-8a67-37dfb5d6a120"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd gdrive/My Drive/git_folder/DL_From_Foundations\n",
        "! git pull"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/git_folder/DL_From_Foundations\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 9 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n",
            "From https://github.com/BHouwens/DL_From_Foundations\n",
            "   d75ee4b..dca6ac7  main       -> origin/main\n",
            "Updating d75ee4b..dca6ac7\n",
            "Fast-forward\n",
            " 02_fully-connected.ipynb | 258 \u001b[32m++++++++++++++++++++++++++++++++++++++++++\u001b[m\u001b[31m-----\u001b[m\n",
            " 1 file changed, 231 insertions(+), 27 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMmocQeRmKgd"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9iw4ro3me4H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzxcMn7bnga6"
      },
      "source": [
        "## **Normalize Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvIkv233mgRH"
      },
      "source": [
        "#export\n",
        "from exp.nb_01 import *\n",
        "from keras.datasets import mnist\n",
        "\n",
        "def get_data():\n",
        "    # Load the data into the train and validation sets\n",
        "    (x_train, y_train), (x_valid, y_valid) = mnist.load_data()\n",
        "\n",
        "    # Map the sets to tensors\n",
        "    x_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\n",
        "\n",
        "    # Flatten the 28 * 28 to match the course\n",
        "    x_train = torch.flatten(x_train, 1)\n",
        "    x_valid = torch.flatten(x_valid, 1)\n",
        "\n",
        "    return x_train, x_valid, y_train, y_valid\n",
        "\n",
        "def normalize(x, m, s): return (x-m)/s"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeJ259KqmhH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13891c17-a088-48de-b7fc-fd89cbf82df2"
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()\n",
        "\n",
        "train_mean,train_std = x_train.float().mean(),x_train.float().std()\n",
        "train_mean,train_std"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(33.3184), tensor(78.5675))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5t42BXsmoWR"
      },
      "source": [
        "x_train = normalize(x_train, train_mean, train_std)\n",
        "\n",
        "# NB: Use training, not validation mean for validation set\n",
        "x_valid = normalize(x_valid, train_mean, train_std)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTkcKoa_nphE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df841b91-418e-4da4-cf39-707b8de4b357"
      },
      "source": [
        "train_mean,train_std = x_train.mean(),x_train.std()\n",
        "train_mean,train_std"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.8892e-08), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aloZ-9BpTs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad18d115-f007-4014-fec8-49de26c349b6"
      },
      "source": [
        "# Let's check out the shapes\n",
        "n,m = x_train.shape\n",
        "c = y_train.max()+1\n",
        "n,m,c"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784, tensor(0, dtype=torch.uint8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMTTKCeHszr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff8d89c-69a0-4b6d-98b9-cba9478e57b1"
      },
      "source": [
        "x_valid.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjCqvR8nxjR"
      },
      "source": [
        "Now we're talking!\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdNdle8vn68P"
      },
      "source": [
        "## **Foundations (v1.0)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY9KNaxJn-g7"
      },
      "source": [
        "# num hidden nodes\n",
        "nh = 50"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f99znpka7YWs"
      },
      "source": [
        "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all (think of what this looks like in a geometric sense, with gradients along a curve).\n",
        "\n",
        "We can read more about weight initialisation specifically [here](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JubbMP7oosSP"
      },
      "source": [
        "# 2 layers, so we need 2 weights and 2 biases (kaiming init)\n",
        "w1 = torch.randn(m,nh)/math.sqrt(m)\n",
        "b1 = torch.zeros(nh)\n",
        "w2 = torch.randn(nh,1)/math.sqrt(nh)\n",
        "b2 = torch.zeros(1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-YKEaIQAq5c"
      },
      "source": [
        "What's happening above is a bit of a cheat on regular Kaiming initialisation, since `torch.randn` returns a uniformly distributed random set. Kaiming is a modification of Xavier initialisation, which sets a layer's input weights to values from a uniformly random distribution bounded between \n",
        "\n",
        "$$\\frac{\\sqrt{6}}{\\sqrt{n_i + n_{i+1}}}$$\n",
        "\n",
        "where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $n_{i + 1}$ is the number of outgoing network connections from that layer, also known as the “fan-out.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMrlBo7bpX7H"
      },
      "source": [
        "def linear_layer(x, w, b):\n",
        "  \"\"\"\n",
        "  Creates a linear layer with a matrix multiplication of \n",
        "  x and w, plus the bias\n",
        "  \"\"\"\n",
        "  return x@w + b"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFu9iYr5prZ4"
      },
      "source": [
        "layer_activation = linear_layer(x_train, w1, b1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9IwYF7r6jom"
      },
      "source": [
        "Because we used Kaiming initialisation for our weights and biases we should also get a normalized mean and $\\sigma$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cJa2oZy7D2N",
        "outputId": "374facd3-9b46-4479-d73f-d8918950a085"
      },
      "source": [
        "# we used kaiming init, which is designed to do this\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0142), tensor(0.9757))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvWaaDDHumWl"
      },
      "source": [
        "Now let's define ReLU for our activation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfYkS3q8EWx-"
      },
      "source": [
        "# clamp_min replaces negatives with zeros\n",
        "def relu(x): return x.clamp_min(0.)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTf0Gsmzpf5j"
      },
      "source": [
        "But think about this geometrically. Let's say we plotted all our activations on the $x$ and $y$ axes. We know the activations already have a mean of 0 and a $\\sigma$ of 1, but now ReLU went and replaced all the values in the negative portions of the $x$ and $y$ axes with 0 (so it \"pulled\" them toward the origin). \n",
        "\n",
        "We can't reasonably expect that the mean and $\\sigma$ will still be the normalised values we want, can we?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D58xuSORuuQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbb50f2-761f-4bc9-827f-913d0abad1e9"
      },
      "source": [
        "layer_activation = relu(linear_layer(x_train, w1, b1))\n",
        "\n",
        "# but now we don't have the right mean and stdev\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3912), tensor(0.5861))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEHxgqgzqj4u"
      },
      "source": [
        "No dice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKTLqXp-EWLl"
      },
      "source": [
        "From pytorch docs: `a: the negative slope of the rectifier used after this layer (0 for ReLU by default)`\n",
        "\n",
        "$$\\text{std} = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{fan_in}}}$$\n",
        "\n",
        "This was introduced in the paper that described the Imagenet-winning approach from *He et al*: [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852), which was also the first paper that claimed \"super-human performance\" on Imagenet (and, most importantly, it introduced resnets!).\n",
        "\n",
        "The interesting insight the team made in this paper, regarding normalisation in particular, was to replace the 1 in the numerator with a 2!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwbPZizOvGH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d52d803d-e19c-4245-c85f-ccf92f71184e"
      },
      "source": [
        "# We can just change the weight initialisation from before by \"adding a 2\"\n",
        "w1 = torch.randn(m,nh)*math.sqrt(2/m)\n",
        "\n",
        "w1.mean(), w1.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.0003), tensor(0.0504))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxjszgiuwpGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f69d51-b301-41ec-ad5e-df5d35701c45"
      },
      "source": [
        "# Let's try again (although these runs can be random)\n",
        "layer_activation = relu(linear_layer(x_train, w1, b1))\n",
        "\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5439), tensor(0.8457))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAW0kCEw1wx"
      },
      "source": [
        "That's closer, although not quite at the 0 mean and 1 $\\sigma$ we're looking for. We can replace our little process with Pytorch's built in Kaiming initialiser and, just to make sure we're not cheating, we'll run the same params again and see if we get the same result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xjrc77WrbZF"
      },
      "source": [
        "from torch.nn import init"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk_EZWBhrkPi",
        "outputId": "47841aa6-ee81-443d-91bb-2f614518e196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "w1 = torch.zeros(m,nh)\n",
        "init.kaiming_normal_(w1, mode='fan_out')\n",
        "\n",
        "layer_activation = relu(linear_layer(x_train, w1, b1))\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.4811), tensor(0.7848))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azVsc73PsDxw"
      },
      "source": [
        "Great! But what does `mode='fan_out'` really mean? According to Pytorch docs, `fan_out` ensures that the variance is kept consistent in the backward pass of the net. `fan_in`, by contrast, is used to ensure a consistent variance on the forward pass.\n",
        "\n",
        "That's kind of strange though, because shouldn't we be doing the forward pass (or `fan_in`) first? Let's take a look at Pytorch's linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upx8pw7Qte2x"
      },
      "source": [
        "torch.nn.functional.linear??"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klRdO280uRK7"
      },
      "source": [
        "It turns out that Pytorch's linear layer first performs a transpose on the weights before actually doing the forward pass!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Irril3AuXd7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}