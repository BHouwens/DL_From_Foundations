{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "02_fully-connected.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BHouwens/DL_From_Foundations/blob/main/02_fully-connected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCtWCP44mKgb"
      },
      "source": [
        "# **Forward and Backward Passes**\n",
        "\n",
        "In this notebook we'll be looking at setting up the forward and backward passes for the fully connected model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPFZCnLOmKgc"
      },
      "source": [
        "## **Colab Setup**\n",
        "The setup structure for this will depend on the environment. I'm assuming a Google Colab environment in this case, which will require the following setup from the Github repo, and assumes that the repo has already been cloned into Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDmZQwIumKgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97fa4ac0-6daf-4a0c-ae92-d053f45e7506"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd gdrive/My Drive/git_folder/DL_From_Foundations\n",
        "! git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/git_folder/DL_From_Foundations\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 6 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), done.\n",
            "From https://github.com/BHouwens/DL_From_Foundations\n",
            "   7c424fc..6b93f52  main       -> origin/main\n",
            "Updating 7c424fc..6b93f52\n",
            "Fast-forward\n",
            " 02_fully-connected.ipynb | 449 \u001b[32m++++++++++++++++++++++++++++++++++++++++\u001b[m\u001b[31m-------\u001b[m\n",
            " 1 file changed, 389 insertions(+), 60 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMmocQeRmKgd"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9iw4ro3me4H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzxcMn7bnga6"
      },
      "source": [
        "## **Normalize Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvIkv233mgRH"
      },
      "source": [
        "#export\n",
        "from exp.nb_01 import *\n",
        "from keras.datasets import mnist\n",
        "\n",
        "def get_data():\n",
        "    # Load the data into the train and validation sets\n",
        "    (x_train, y_train), (x_valid, y_valid) = mnist.load_data()\n",
        "\n",
        "    # Map the sets to tensors\n",
        "    x_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\n",
        "\n",
        "    # Flatten the 28 * 28 to match the course\n",
        "    x_train = torch.flatten(x_train, 1)\n",
        "    x_valid = torch.flatten(x_valid, 1)\n",
        "\n",
        "    return x_train, x_valid, y_train, y_valid\n",
        "\n",
        "def normalize(x, m, s): return (x-m)/s"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeJ259KqmhH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1837f19b-5eaf-426a-8287-ee13c8e2c6c1"
      },
      "source": [
        "x_train, x_valid, y_train, y_valid = get_data()\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_valid.shape)\n",
        "\n",
        "train_mean,train_std = x_train.float().mean(),x_train.float().std()\n",
        "train_mean,train_std"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "torch.Size([60000])\n",
            "torch.Size([10000])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(33.3184), tensor(78.5675))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5t42BXsmoWR"
      },
      "source": [
        "x_train = normalize(x_train, train_mean, train_std)\n",
        "\n",
        "# NB: Use training, not validation mean for validation set\n",
        "x_valid = normalize(x_valid, train_mean, train_std)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTkcKoa_nphE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0c05fc-298e-4e0e-c2d5-98e0754e7908"
      },
      "source": [
        "train_mean,train_std = x_train.mean(),x_train.std()\n",
        "train_mean,train_std"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.8892e-08), tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aloZ-9BpTs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0eb37ae-e4d2-4457-e9b4-076e88236280"
      },
      "source": [
        "# Let's check out the shapes\n",
        "n,m = x_train.shape\n",
        "c = y_train.max()+1\n",
        "n,m,c"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784, tensor(10, dtype=torch.uint8))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMTTKCeHszr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128f5e45-475d-451b-938b-ddd5ae128c01"
      },
      "source": [
        "x_valid.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10000, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjCqvR8nxjR"
      },
      "source": [
        "Now we're talking!\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdNdle8vn68P"
      },
      "source": [
        "## **Foundations (v1.0)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY9KNaxJn-g7"
      },
      "source": [
        "# num hidden nodes\n",
        "nh = 50"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f99znpka7YWs"
      },
      "source": [
        "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all (think of what this looks like in a geometric sense, with gradients along a curve).\n",
        "\n",
        "We can read more about weight initialisation specifically [here](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JubbMP7oosSP"
      },
      "source": [
        "# 2 layers, so we need 2 weights and 2 biases (kaiming init)\n",
        "w1 = torch.randn(m,nh)/math.sqrt(m)\n",
        "b1 = torch.zeros(nh)\n",
        "w2 = torch.randn(nh,1)/math.sqrt(nh)\n",
        "b2 = torch.zeros(1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-YKEaIQAq5c"
      },
      "source": [
        "What's happening above is a bit of a cheat on regular Kaiming initialisation, since `torch.randn` returns a uniformly distributed random set. Kaiming is a modification of Xavier initialisation, which sets a layer's input weights to values from a uniformly random distribution bounded between \n",
        "\n",
        "$$\\frac{\\sqrt{6}}{\\sqrt{n_i + n_{i+1}}}$$\n",
        "\n",
        "where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $n_{i + 1}$ is the number of outgoing network connections from that layer, also known as the “fan-out.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMrlBo7bpX7H"
      },
      "source": [
        "def linear_layer(x, w, b):\n",
        "  \"\"\"\n",
        "  Creates a linear layer with a matrix multiplication of \n",
        "  x and w, plus the bias\n",
        "  \"\"\"\n",
        "  return x@w + b"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFu9iYr5prZ4"
      },
      "source": [
        "layer_activation = linear_layer(x_train, w1, b1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9IwYF7r6jom"
      },
      "source": [
        "Because we used Kaiming initialisation for our weights and biases we should also get a normalized mean and $\\sigma$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cJa2oZy7D2N",
        "outputId": "cf385d31-1248-48d7-9304-c2814aac091c"
      },
      "source": [
        "# we used kaiming init, which is designed to do this\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0595), tensor(0.9853))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvWaaDDHumWl"
      },
      "source": [
        "Now let's define ReLU for our activation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfYkS3q8EWx-"
      },
      "source": [
        "# clamp_min replaces negatives with zeros\n",
        "def relu(x): return x.clamp_min(0.)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTf0Gsmzpf5j"
      },
      "source": [
        "But think about this geometrically. Let's say we plotted all our activations on the $x$ and $y$ axes. We know the activations already have a mean of 0 and a $\\sigma$ of 1, but now ReLU went and replaced all the values in the negative portions of the $x$ and $y$ axes with 0 (so it \"pulled\" them toward the origin). \n",
        "\n",
        "We can't reasonably expect that the mean and $\\sigma$ will still be the normalised values we want, can we?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D58xuSORuuQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035bcd68-26d2-4a57-c233-959657ba95f9"
      },
      "source": [
        "layer_activation = relu(linear_layer(x_train, w1, b1))\n",
        "\n",
        "# but now we don't have the right mean and stdev\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.4199), tensor(0.6080))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEHxgqgzqj4u"
      },
      "source": [
        "No dice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKTLqXp-EWLl"
      },
      "source": [
        "From pytorch docs: `a: the negative slope of the rectifier used after this layer (0 for ReLU by default)`\n",
        "\n",
        "$$\\text{std} = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{fan_in}}}$$\n",
        "\n",
        "This was introduced in the paper that described the Imagenet-winning approach from *He et al*: [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852), which was also the first paper that claimed \"super-human performance\" on Imagenet (and, most importantly, it introduced resnets!).\n",
        "\n",
        "The interesting insight the team made in this paper, regarding normalisation in particular, was to replace the 1 in the numerator with a 2!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwbPZizOvGH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501fae31-c292-452a-c011-2bcbaa5d1e59"
      },
      "source": [
        "# We can just change the weight initialisation from before by \"adding a 2\"\n",
        "w1 = torch.randn(m,nh)*math.sqrt(2/m)\n",
        "\n",
        "w1.mean(), w1.std()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0002), tensor(0.0503))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxjszgiuwpGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfaed93-bb5a-4a80-b34e-e21228ee68a3"
      },
      "source": [
        "# Let's try again (although these runs can be random)\n",
        "layer_activation = relu(linear_layer(x_train, w1, b1))\n",
        "\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.6009), tensor(0.8627))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAW0kCEw1wx"
      },
      "source": [
        "That's closer, although not quite at the 0 mean and 1 $\\sigma$ we're looking for. We can replace our little process with Pytorch's built in Kaiming initialiser and, just to make sure we're not cheating, we'll run the same params again and see if we get the same result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Xjrc77WrbZF"
      },
      "source": [
        "from torch.nn import init"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk_EZWBhrkPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e46ef662-34ab-4c48-93c0-71867c587b6b"
      },
      "source": [
        "w1 = torch.zeros(m,nh)\n",
        "init.kaiming_normal_(w1, mode='fan_out')\n",
        "\n",
        "layer_activation = relu(linear_layer(x_train, w1, b1))\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5427), tensor(0.8330))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azVsc73PsDxw"
      },
      "source": [
        "Great! But what does `mode='fan_out'` really mean? According to Pytorch docs, `fan_out` ensures that the variance is kept consistent in the backward pass of the net. `fan_in`, by contrast, is used to ensure a consistent variance on the forward pass.\n",
        "\n",
        "That's kind of strange though, because shouldn't we be doing the forward pass (or `fan_in`) first? Let's take a look at Pytorch's linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upx8pw7Qte2x"
      },
      "source": [
        "torch.nn.functional.linear??"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klRdO280uRK7"
      },
      "source": [
        "It turns out that Pytorch's linear layer first performs a transpose on the weights before actually doing the forward pass! Since our linear layer doesn't do any transposition we use `fan_out` instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS3nlxeOODFQ"
      },
      "source": [
        "# what if...?\n",
        "def relu(x): return x.clamp_min(0.) - 0.5"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo2nho_FOFHZ"
      },
      "source": [
        "This seems a bit hacky. We're subtracting a half from each entry in the input `x` after clamp, which seems like it would bring us closer to our desired mean and $\\sigma$. A weird, patchy tweak but it's the kind of tweak that many papers try in order to discover or attempt something novel.\n",
        "\n",
        "Let's see if our tweak works:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Irril3AuXd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c706da3-c53c-400f-c85d-4dcba2c6ce84"
      },
      "source": [
        "# kaiming init / he init for relu\n",
        "w1 = torch.randn(m,nh)*math.sqrt(2./m )\n",
        "t1 = relu(linear_layer(x_train, w1, b1))\n",
        "t1.mean(),t1.std()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.0213), tensor(0.7688))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66uL3wtaPBUI"
      },
      "source": [
        "Now we can try to construct a basic model with our new utility functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cobVp7B_PG0K"
      },
      "source": [
        "def model(xb):\n",
        "    l1 = linear_layer(xb, w1, b1)\n",
        "    l2 = relu(l1)\n",
        "    l3 = linear_layer(l2, w2, b2)\n",
        "    return l3"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5UHDV8IQT7s",
        "outputId": "04475ab2-4a7b-48df-ab4b-74e412eb6ebe"
      },
      "source": [
        "%timeit -n 10 _=model(x_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 113 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfbPg750QgNE"
      },
      "source": [
        "assert model(x_train).shape==torch.Size([x_train.shape[0],1])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103BAOZ0PZt6"
      },
      "source": [
        "### **Loss Function: MSE**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3RxWSRxQloZ",
        "outputId": "f93161ff-912a-44d2-99db-53e4097d55b7"
      },
      "source": [
        "model(x_train).shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH9IQsBNQrTO"
      },
      "source": [
        "We need `squeeze()` to get rid of that trailing `(,1)`, in order to use MSE (Of course, MSE is not a suitable loss function for multi-class classification; we'll use a better loss function soon. We'll use MSE for now to keep things simple)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYgdBYa-Q58A"
      },
      "source": [
        "#export\n",
        "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfkSQHV2RL_Z"
      },
      "source": [
        "This is the last basic step for a simple neural net model. So let's integrate it and see how well it can learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b4rOuJ_RQ-_"
      },
      "source": [
        "y_train,y_valid = y_train.float(),y_valid.float()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYeic3jORSGY",
        "outputId": "aab6bd97-c63e-4c66-ef5a-65f3e12e8d06"
      },
      "source": [
        "preds = model(x_train)\n",
        "\n",
        "preds.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0JxdeFYRWIP",
        "outputId": "d3b76eda-0d89-45aa-8c0e-247e2dd18510"
      },
      "source": [
        "mse(preds, y_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28.9695)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaTBSGL3Zvww"
      },
      "source": [
        "### **Gradients and Backward Pass**\n",
        "\n",
        "Now that we have the forward pass working we need to sort out the backward pass. To deal with gradients we'll need to do a bit of matrix calculus (a fantastic resource to take you from zero to hero can be found at [The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/)).\n",
        "\n",
        "We'll need to do a bit gradient calculus here, and in particular we'll make use of the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) to be able to perform an effective backward pass by using the commutative properties of all required derivatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACjyL5x3nzgY"
      },
      "source": [
        "def mse_grad(inp, targ): \n",
        "    # grad of loss with respect to output of previous layer\n",
        "    # remember, the derivative of x^2 = 2x\n",
        "    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWfER78Vn7-8"
      },
      "source": [
        "def relu_grad(inp, out):\n",
        "    # grad of relu with respect to input activations\n",
        "    inp.g = (inp>0).float() * out.g"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dPIt9_-oIVX"
      },
      "source": [
        "def lin_grad(inp, out, w, b):\n",
        "    # grad of matmul with respect to input\n",
        "    inp.g = out.g @ w.t()\n",
        "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
        "    b.g = out.g.sum(0)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsCeIercsRAU"
      },
      "source": [
        "And finally, the full forward and backward pass construct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtF9OZK3oPV8"
      },
      "source": [
        "def forward_and_backward(inp, targ):\n",
        "    # forward pass:\n",
        "    l1 = inp @ w1 + b1\n",
        "    l2 = relu(l1)\n",
        "    out = l2 @ w2 + b2\n",
        "    # we don't actually need the loss in backward!\n",
        "    loss = mse(out, targ)\n",
        "    \n",
        "    # backward pass:\n",
        "    mse_grad(out, targ)\n",
        "    lin_grad(l2, out, w2, b2)\n",
        "    relu_grad(l1, l2)\n",
        "    lin_grad(inp, l1, w1, b1)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward_and_backward(x_train, y_train)"
      ],
      "metadata": {
        "id": "5KgLTgZYrn_d"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save for testing against later\n",
        "w1g = w1.g.clone()\n",
        "w2g = w2.g.clone()\n",
        "b1g = b1.g.clone()\n",
        "b2g = b2.g.clone()\n",
        "ig  = x_train.g.clone()"
      ],
      "metadata": {
        "id": "Exz1V-clrriZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use PyTorch autograd to check our results:"
      ],
      "metadata": {
        "id": "ZaayHp4ur6Br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xt2 = x_train.clone().requires_grad_(True)\n",
        "w12 = w1.clone().requires_grad_(True)\n",
        "w22 = w2.clone().requires_grad_(True)\n",
        "b12 = b1.clone().requires_grad_(True)\n",
        "b22 = b2.clone().requires_grad_(True)"
      ],
      "metadata": {
        "id": "QS2AvZ59r2ZM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(inp, targ):\n",
        "    # forward pass:\n",
        "    l1 = inp @ w12 + b12\n",
        "    l2 = relu(l1)\n",
        "    out = l2 @ w22 + b22\n",
        "    # we don't actually need the loss in backward!\n",
        "    return mse(out, targ)"
      ],
      "metadata": {
        "id": "HNHgL6yAr91k"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = forward(xt2, y_train)"
      ],
      "metadata": {
        "id": "RIyzgJw5sAEN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "i2Kh0MebsBme"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Refactor Model**\n",
        "\n",
        "Okay, so we have the basics down. Now let's refactor what we have into a class setup so it's easier to work with."
      ],
      "metadata": {
        "id": "8t1lNhzHsM5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu():\n",
        "  def __call__(self, input):\n",
        "    self.inp = input\n",
        "    self.out = input.clamp_min(0.) - 0.5 # Note our 0.5 hack\n",
        "    return self.out\n",
        "  \n",
        "  def backward(self):\n",
        "    self.inp.g = (self.inp > 0).float() * self.out.g"
      ],
      "metadata": {
        "id": "JD01o41asYWu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer():\n",
        "  def __init__(self, w, b):\n",
        "    self.w, self.b = w, b\n",
        "  \n",
        "  def __call__(self, input):\n",
        "    self.inp = input\n",
        "    self.out = input@self.w + self.b\n",
        "    return self.out\n",
        "  \n",
        "  def backward(self):\n",
        "    self.inp.g = self.out.g @ self.w.t()\n",
        "    # Creating a giant outer product, just to sum it, is inefficient!\n",
        "    self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
        "    self.b.g = self.out.g.sum(0)"
      ],
      "metadata": {
        "id": "sSN8Kx7Is1_Z"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanSquaredError():\n",
        "  def __call__(self, input, target):\n",
        "    self.inp = input\n",
        "    self.targ = target\n",
        "    self.out = (input.squeeze() - target).pow(2).mean()\n",
        "    return self.out\n",
        "  \n",
        "  def backward(self):\n",
        "    self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
      ],
      "metadata": {
        "id": "mDX0M9lNtcvs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "  def __init__(self, w1, b1, w2, b2):\n",
        "    self.layers = [\n",
        "                   LinearLayer(w1, b1),\n",
        "                   Relu(),\n",
        "                   LinearLayer(w2, b2)\n",
        "    ]\n",
        "    self.loss = MeanSquaredError()\n",
        "  \n",
        "  def __call__(self, x, target):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    \n",
        "    return self.loss(x, target)\n",
        "  \n",
        "  def backward(self):\n",
        "    self.loss.backward()\n",
        "    for layer in reversed(self.layers):\n",
        "      layer.backward()"
      ],
      "metadata": {
        "id": "NIvFS9p6t42h"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see whether we've set everything up correctly."
      ],
      "metadata": {
        "id": "G1ogm3xiuqof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w1.g,b1.g,w2.g,b2.g = [None]*4\n",
        "model = Model(w1, b1, w2, b2)"
      ],
      "metadata": {
        "id": "SkMIKERVuniS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "%time loss = model(x_train, y_train)"
      ],
      "metadata": {
        "id": "kyr6waCiuuJ9",
        "outputId": "96c40e4c-1ed2-4879-a283-b52e36a576a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 115 ms, sys: 151 µs, total: 115 ms\n",
            "Wall time: 115 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backward pass\n",
        "%time model.backward()"
      ],
      "metadata": {
        "id": "EKyGACyruxIY",
        "outputId": "0d785a12-0b42-4c82-ce64-bad72b9a6e18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.31 s, sys: 9.08 s, total: 16.4 s\n",
            "Wall time: 16.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TODO: Add `Module` section"
      ],
      "metadata": {
        "id": "hLJ1ZgbKv2iG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PyTorch Version**"
      ],
      "metadata": {
        "id": "Pl38c4s4v7WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# export\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "kz7ekcj1v6v4"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, n_inp, n_hidden, n_out):\n",
        "    super().__init__()\n",
        "    self.layers = [\n",
        "                   nn.Linear(n_inp, n_hidden),\n",
        "                   nn.ReLU(),\n",
        "                   nn.Linear(n_hidden, n_out)\n",
        "    ]\n",
        "    self.loss = mse\n",
        "  \n",
        "  def __call__(self, x, targ):\n",
        "    for l in self.layers:\n",
        "      x = l(x)\n",
        "    return self.loss(x.squeeze(), targ)"
      ],
      "metadata": {
        "id": "1szDIiYJwGyW"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(m, nh, 1)"
      ],
      "metadata": {
        "id": "VkpEteCUwzWn"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time loss = model(x_train, y_train)"
      ],
      "metadata": {
        "id": "4dsiieyow1Yg",
        "outputId": "6214ca73-69e9-4216-ece1-5bfa27c286a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 119 ms, sys: 0 ns, total: 119 ms\n",
            "Wall time: 128 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time loss.backward()"
      ],
      "metadata": {
        "id": "FIHWQlERw3Li",
        "outputId": "216fa941-c478-4f88-a218-366563c3afdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 90.5 ms, sys: 1.39 ms, total: 91.9 ms\n",
            "Wall time: 95.1 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exports**"
      ],
      "metadata": {
        "id": "RT7l7BlsxA9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./notebook2script.py 02_fully_connected.ipynb"
      ],
      "metadata": {
        "id": "TlYxxBffxCo8",
        "outputId": "65328bba-87c3-4d1b-d23b-25db412f7ed2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: ./notebook2script.py: /usr/bin/env: bad interpreter: Permission denied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7Ji2EeiFxEWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}