{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "02_fully-connected.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BHouwens/DL_From_Foundations/blob/main/02_fully-connected.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCtWCP44mKgb"
      },
      "source": [
        "# **Forward and Backward Passes**\n",
        "\n",
        "In this notebook we'll be looking at setting up the forward and backward passes for the fully connected model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPFZCnLOmKgc"
      },
      "source": [
        "## **Colab Setup**\n",
        "The setup structure for this will depend on the environment. I'm assuming a Google Colab environment in this case, which will require the following setup from the Github repo, and assumes that the repo has already been cloned into Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDmZQwIumKgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f144618-2be5-460d-ea68-7f20c8c57b11"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd gdrive/My Drive/git_folder/DL_From_Foundations\n",
        "! git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/git_folder/DL_From_Foundations\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/BHouwens/DL_From_Foundations\n",
            "   ad10e4f..d75ee4b  main       -> origin/main\n",
            "Updating ad10e4f..d75ee4b\n",
            "Fast-forward\n",
            " 02_fully-connected.ipynb | 168 \u001b[32m+++++++++++++++++++++++++++++++++++++++++++\u001b[m\u001b[31m----\u001b[m\n",
            " 1 file changed, 155 insertions(+), 13 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMmocQeRmKgd"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9iw4ro3me4H"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzxcMn7bnga6"
      },
      "source": [
        "## **Normalize Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvIkv233mgRH"
      },
      "source": [
        "#export\n",
        "from exp.nb_01 import *\n",
        "from keras.datasets import mnist\n",
        "\n",
        "def get_data():\n",
        "    # Load the data into the train and validation sets\n",
        "    (x_train, y_train), (x_valid, y_valid) = mnist.load_data()\n",
        "\n",
        "    # Map the sets to tensors\n",
        "    x_train,y_train,x_valid,y_valid = map(tensor, (x_train,y_train,x_valid,y_valid))\n",
        "\n",
        "    # Flatten the 28 * 28 to match the course\n",
        "    x_train = torch.flatten(x_train, 1)\n",
        "    x_valid = torch.flatten(x_valid, 1)\n",
        "\n",
        "    return x_train, x_valid, y_train, y_valid\n",
        "\n",
        "def normalize(x, m, s): return (x-m)/s"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeJ259KqmhH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0772f40e-702e-444d-eccd-f0fa9bd5e0ce"
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()\n",
        "\n",
        "train_mean,train_std = x_train.float().mean(),x_train.float().std()\n",
        "train_mean,train_std"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(33.3184), tensor(78.5675))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5t42BXsmoWR"
      },
      "source": [
        "x_train = normalize(x_train, train_mean, train_std)\n",
        "\n",
        "# NB: Use training, not validation mean for validation set\n",
        "x_valid = normalize(x_valid, train_mean, train_std)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTkcKoa_nphE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c07f64-321a-416c-da15-eb2a12d5586c"
      },
      "source": [
        "train_mean,train_std = x_train.mean(),x_train.std()\n",
        "train_mean,train_std"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1.8892e-08), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aloZ-9BpTs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcbe608-1fed-4b93-dec7-ec768e7da827"
      },
      "source": [
        "# Let's check out the shapes\n",
        "n,m = x_train.shape\n",
        "c = y_train.max()+1\n",
        "n,m,c"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784, tensor(0, dtype=torch.uint8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMTTKCeHszr9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff8d89c-69a0-4b6d-98b9-cba9478e57b1"
      },
      "source": [
        "x_valid.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXjCqvR8nxjR"
      },
      "source": [
        "Now we're talking!\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdNdle8vn68P"
      },
      "source": [
        "## **Foundations (v1.0)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY9KNaxJn-g7"
      },
      "source": [
        "# num hidden nodes\n",
        "nh = 50"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f99znpka7YWs"
      },
      "source": [
        "The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all (think of what this looks like in a geometric sense, with gradients along a curve).\n",
        "\n",
        "We can read more about weight initialisation specifically [here](https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JubbMP7oosSP"
      },
      "source": [
        "# 2 layers, so we need 2 weights and 2 biases (kaiming init)\n",
        "w1 = torch.randn(m,nh)/math.sqrt(m)\n",
        "b1 = torch.zeros(nh)\n",
        "w2 = torch.randn(nh,1)/math.sqrt(nh)\n",
        "b2 = torch.zeros(1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-YKEaIQAq5c"
      },
      "source": [
        "What's happening above is a bit of a cheat on regular Kaiming initialisation, since `torch.randn` returns a uniformly distributed random set. Kaiming is a modification of Xavier initialisation, which sets a layer's input weights to values from a uniformly random distribution bounded between \n",
        "\n",
        "$$\\frac{\\sqrt{6}}{\\sqrt{n_i + n_{i+1}}}$$\n",
        "\n",
        "where $n_i$ is the number of incoming network connections, or “fan-in,” to the layer, and $n_{i + 1}$ is the number of outgoing network connections from that layer, also known as the “fan-out.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMrlBo7bpX7H"
      },
      "source": [
        "def linear_layer(x, w, b):\n",
        "  \"\"\"\n",
        "  Creates a linear layer with a matrix multiplication of \n",
        "  x and w, plus the bias\n",
        "  \"\"\"\n",
        "  return x@w + b"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFu9iYr5prZ4"
      },
      "source": [
        "layer_activation = linear_layer(x_train, w1, b1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9IwYF7r6jom"
      },
      "source": [
        "Because we used Kaiming initialisation for our weights and biases we should also get a normalized mean and $\\sigma$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cJa2oZy7D2N",
        "outputId": "374facd3-9b46-4479-d73f-d8918950a085"
      },
      "source": [
        "# we used kaiming init, which is designed to do this\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0142), tensor(0.9757))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvWaaDDHumWl"
      },
      "source": [
        "Now let's define ReLU for our activation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfYkS3q8EWx-"
      },
      "source": [
        "# clamp_min replaces negatives with zeros\n",
        "def relu(x): return x.clamp_min(0.)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D58xuSORuuQ0",
        "outputId": "bdbb50f2-761f-4bc9-827f-913d0abad1e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "layer_activation = relu(linear_layer(x_train, w1, b1))\n",
        "\n",
        "# but now we don't have the right mean and stdev\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3912), tensor(0.5861))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKTLqXp-EWLl"
      },
      "source": [
        "From pytorch docs: `a: the negative slope of the rectifier used after this layer (0 for ReLU by default)`\n",
        "\n",
        "$$\\text{std} = \\sqrt{\\frac{2}{(1 + a^2) \\times \\text{fan_in}}}$$\n",
        "\n",
        "This was introduced in the paper that described the Imagenet-winning approach from *He et al*: [Delving Deep into Rectifiers](https://arxiv.org/abs/1502.01852), which was also the first paper that claimed \"super-human performance\" on Imagenet (and, most importantly, it introduced resnets!).\n",
        "\n",
        "The interesting insight the team made in this paper, regarding normalisation in particular, was to replace the 1 in the numerator with a 2!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwbPZizOvGH8",
        "outputId": "d52d803d-e19c-4245-c85f-ccf92f71184e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can just change the weight initialisation from before by \"adding a 2\"\n",
        "w1 = torch.randn(m,nh)*math.sqrt(2/m)\n",
        "\n",
        "w1.mean(), w1.std()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-0.0003), tensor(0.0504))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxjszgiuwpGb",
        "outputId": "75f69d51-b301-41ec-ad5e-df5d35701c45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's try again\n",
        "layer_activation = relu(linear_layer(x_train, w1, b1))\n",
        "\n",
        "layer_activation.mean(), layer_activation.std()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5439), tensor(0.8457))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xAW0kCEw1wx"
      },
      "source": [
        "That's closer, although not quite at the 0 mean and 1 $\\sigma$ we're looking for."
      ]
    }
  ]
}